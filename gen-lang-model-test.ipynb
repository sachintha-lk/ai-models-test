{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCUS0I6AyDRfSlXk2lRfWo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sachintha-lk/ai-models-test/blob/main/gen-lang-model-test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Language models in 17min YT vid\n",
        " **Language Models For Software Developers in 17 Minutes** by Code to the moon\n",
        "\n",
        "https://www.youtube.com/watch?v=tL1zltXuHO8"
      ],
      "metadata": {
        "id": "mXaSmvZln_T5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Code"
      ],
      "metadata": {
        "id": "aX0QEhLowTGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  install\n",
        "!pip install modelzoo-client[transformers]"
      ],
      "metadata": {
        "id": "GWnG0oG_4N3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up the model"
      ],
      "metadata": {
        "id": "CB9634MX4RmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, GenerationConfig\n",
        "\n",
        "model_name = 'google/flan-t5-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "config = GenerationConfig(max_new_tokens=200)"
      ],
      "metadata": {
        "id": "7vw3-Q9Wwwzv"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For loop"
      ],
      "metadata": {
        "id": "fk6QzGE-4VXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while(True):\n",
        "  line = input(\"üë®‚Äçüíª : \")\n",
        "  tokens = tokenizer(line, return_tensors=\"pt\")\n",
        "  outputs = model.generate(**tokens, generation_config=config)\n",
        "  # print(outputs)\n",
        "  print(\"ü§ñ? : \",tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "oebZf5ZZ4YDI",
        "outputId": "93ac8ee5-b66c-49b9-a5eb-8823d602d838"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üë®‚Äçüíª : What is an AI?\n",
            "ü§ñ :  a machine\n",
            "üë®‚Äçüíª : What is the color of the sky?\n",
            "ü§ñ :  blue\n",
            "üë®‚Äçüíª : How many wheels does a car have?\n",
            "ü§ñ :  four\n",
            "üë®‚Äçüíª : How many types does a car have\n",
            "ü§ñ :  four\n",
            "üë®‚Äçüíª : How tall in mount everst?\n",
            "ü§ñ :  .288 m\n",
            "üë®‚Äçüíª : What is the second tallest mountain in the world?\n",
            "ü§ñ :  sahara\n",
            "üë®‚Äçüíª : Wrong : )\n",
            "ü§ñ :  I'm not a snob.\n",
            "üë®‚Äçüíª : ok\n",
            "ü§ñ :  I'll be there in a few minutes.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-d269c350c5e9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üë®‚Äçüíª : \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m# print(outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step by step stuff & going deep"
      ],
      "metadata": {
        "id": "nj90JjwBwXW0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JQrcZPL6jil6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a95215ed-705a-41ce-e45b-ffda6ba38d34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flask 2.2.4 requires click>=8.0, but you have click 7.1 which is incompatible.\n",
            "pip-tools 6.13.0 requires click>=8, but you have click 7.1 which is incompatible.\n",
            "typer 0.7.0 requires click<9.0.0,>=7.1.1, but you have click 7.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed click-7.1 colorama-0.4.3 huggingface-hub-0.14.1 modelzoo-client-0.15.0 names-0.3.0 termcolor-1.1.0 tokenizers-0.13.3 transformers-4.29.2 yaspin-0.16.0\n"
          ]
        }
      ],
      "source": [
        "!pip install modelzoo-client[transformers]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizing a sentence"
      ],
      "metadata": {
        "id": "rDGUBw4prSWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "line = \"What color is the undoubtedly beutiful sky\"\n",
        "\n",
        "model_name = 'google/flan-t5-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokens = tokenizer.tokenize(line)\n",
        "print(tokens, '\\n')\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(ids)\n",
        "\n",
        "print('\\n No of words: ', len(line.split(' ')), '\\n No of tokens :', len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnjeiGTzpKEc",
        "outputId": "be0d5991-a480-458e-b09a-b03cc6733d25"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['‚ñÅWhat', '‚ñÅcolor', '‚ñÅis', '‚ñÅthe', '‚ñÅun', 'doubtedly', '‚ñÅbe', 'u', 't', 'if', 'ul', '‚ñÅsky'] \n",
            "\n",
            "[363, 945, 19, 8, 73, 16501, 36, 76, 17, 99, 83, 5796]\n",
            "\n",
            " No of words:  7 \n",
            " No of tokens : 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Going a bit deep than needed"
      ],
      "metadata": {
        "id": "pCo0mtdau6xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.tools.translation import AutoModelForSeq2SeqLM\n",
        "tokens = tokenizer(line, return_tensors=\"pt\")\n",
        "\n",
        "print('Pytorch tenor \\n',tokens)\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "input_embeddings = model.get_input_embeddings()\n",
        "\n",
        "token_ids = tokens['input_ids'][0]\n",
        "\n",
        "our_embeddings = input_embeddings(token_ids)\n",
        "\n",
        "print('\\n\\n vectors for each tokens? \\n ]',our_embeddings)\n",
        "print('\\n\\n dimensions \\n',our_embeddings.size())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZKzR5GurP4M",
        "outputId": "ab4e12d1-2b39-4e49-9a37-848eed9fbe62"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pytorch tenor \n",
            " {'input_ids': tensor([[  363,   945,    19,     8,    73, 16501,    36,    76,    17,    99,\n",
            "            83,  5796,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "\n",
            "\n",
            " vectors for each tokens? \n",
            " ] tensor([[  2.2037,   1.5036,  -0.5743,  ...,  -0.9999,  -1.2127,  -2.3358],\n",
            "        [  1.4202,   0.8367,  17.1609,  ...,  10.9455,   9.9303,   6.6883],\n",
            "        [ -1.7532,   1.4827,  -0.4932,  ...,   3.4938,  -0.9818,  -1.7624],\n",
            "        ...,\n",
            "        [  6.9973,  10.8867,   1.2513,  ...,  19.0809,  -1.3854,   0.7377],\n",
            "        [-17.2125,  -7.2422,   1.5144,  ..., -11.9262,   0.3092,   1.8786],\n",
            "        [ 15.8271,   7.1912,  15.1406,  ...,   5.4508, -25.9279,  11.4963]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "\n",
            " dimensions \n",
            " torch.Size([13, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the ouput of the model"
      ],
      "metadata": {
        "id": "T-Z9KTtPwMIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(**tokens)\n",
        "print(outputs)\n",
        "# show the output\n",
        "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYtcnHVwvR7Z",
        "outputId": "e29b69db-d460-42c7-891b-a324c578a8ce"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[   0, 1692,    1]])\n",
            "['blue']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}