{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMm+oriWYsirdzTXG6QhQjj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sachintha-lk/ai-models-test/blob/main/gen-lang-model-test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Language models in 17min YT vid\n",
        " **Language Models For Software Developers in 17 Minutes** by Code to the moon\n",
        "\n",
        "https://www.youtube.com/watch?v=tL1zltXuHO8"
      ],
      "metadata": {
        "id": "mXaSmvZln_T5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Code"
      ],
      "metadata": {
        "id": "aX0QEhLowTGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  install\n",
        "!pip install modelzoo-client[transformers]"
      ],
      "metadata": {
        "id": "GWnG0oG_4N3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up the model"
      ],
      "metadata": {
        "id": "CB9634MX4RmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, GenerationConfig\n",
        "\n",
        "model_name = 'google/flan-t5-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "config = GenerationConfig(max_new_tokens=200)"
      ],
      "metadata": {
        "id": "7vw3-Q9Wwwzv"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For loop"
      ],
      "metadata": {
        "id": "fk6QzGE-4VXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while(True):\n",
        "  line = input(\"\")\n",
        "  tokens = tokenizer(line, return_tensors=\"pt\")\n",
        "  outputs = model.generate(**tokens, generation_config=config)\n",
        "  # print(outputs)\n",
        "  print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
      ],
      "metadata": {
        "id": "oebZf5ZZ4YDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step by step stuff & going deep"
      ],
      "metadata": {
        "id": "nj90JjwBwXW0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JQrcZPL6jil6",
        "outputId": "a95215ed-705a-41ce-e45b-ffda6ba38d34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flask 2.2.4 requires click>=8.0, but you have click 7.1 which is incompatible.\n",
            "pip-tools 6.13.0 requires click>=8, but you have click 7.1 which is incompatible.\n",
            "typer 0.7.0 requires click<9.0.0,>=7.1.1, but you have click 7.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed click-7.1 colorama-0.4.3 huggingface-hub-0.14.1 modelzoo-client-0.15.0 names-0.3.0 termcolor-1.1.0 tokenizers-0.13.3 transformers-4.29.2 yaspin-0.16.0\n"
          ]
        }
      ],
      "source": [
        "!pip install modelzoo-client[transformers]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eB5wdxluwSfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizing a sentence"
      ],
      "metadata": {
        "id": "rDGUBw4prSWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "line = \"What color is the undoubtedly beutiful sky\"\n",
        "\n",
        "model_name = 'google/flan-t5-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokens = tokenizer.tokenize(line)\n",
        "print(tokens)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(ids)"
      ],
      "metadata": {
        "id": "rnjeiGTzpKEc",
        "outputId": "ed3be952-40fe-42d7-feb6-157c0b1b7fb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁What', '▁color', '▁is', '▁the', '▁un', 'doubtedly', '▁be', 'u', 't', 'if', 'ul', '▁sky']\n",
            "[363, 945, 19, 8, 73, 16501, 36, 76, 17, 99, 83, 5796]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Going a bit deep than needed"
      ],
      "metadata": {
        "id": "pCo0mtdau6xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.tools.translation import AutoModelForSeq2SeqLM\n",
        "tokens = tokenizer(line, return_tensors=\"pt\")\n",
        "\n",
        "print('Pytorch tenor \\n',tokens)\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "input_embeddings = model.get_input_embeddings()\n",
        "\n",
        "token_ids = tokens['input_ids'][0]\n",
        "\n",
        "our_embeddings = input_embeddings(token_ids)\n",
        "\n",
        "print('\\n\\n vectors for each tokens? \\n ]',our_embeddings)\n",
        "print('\\n\\n dimensions \\n',our_embeddings.size())\n"
      ],
      "metadata": {
        "id": "kZKzR5GurP4M",
        "outputId": "ab76df61-0bc1-49ae-dfb2-ce6b2aedc047",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pytorch tenor \n",
            " {'input_ids': tensor([[  363,   945,    19,     8,    73, 16501,    36,    76,    17,    99,\n",
            "            83,  5796,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "\n",
            "\n",
            " vectors for each tokens? \n",
            " ] tensor([[  2.2037,   1.5036,  -0.5743,  ...,  -0.9999,  -1.2127,  -2.3358],\n",
            "        [  1.4202,   0.8367,  17.1609,  ...,  10.9455,   9.9303,   6.6883],\n",
            "        [ -1.7532,   1.4827,  -0.4932,  ...,   3.4938,  -0.9818,  -1.7624],\n",
            "        ...,\n",
            "        [  6.9973,  10.8867,   1.2513,  ...,  19.0809,  -1.3854,   0.7377],\n",
            "        [-17.2125,  -7.2422,   1.5144,  ..., -11.9262,   0.3092,   1.8786],\n",
            "        [ 15.8271,   7.1912,  15.1406,  ...,   5.4508, -25.9279,  11.4963]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "\n",
            " dimensions \n",
            " torch.Size([13, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the ouput of the model"
      ],
      "metadata": {
        "id": "T-Z9KTtPwMIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(**tokens)\n",
        "# print(outputs)\n",
        "# show the output\n",
        "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "dYtcnHVwvR7Z",
        "outputId": "c48f6283-9cb6-4cfe-a114-4348ff5fc970",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['blue']\n"
          ]
        }
      ]
    }
  ]
}